from PIL import Image
import os
import numpy as np
from copy import deepcopy
from torch.utils.data import Dataset
from data.data_utils import subsample_instances
from config import clear_10_root
from torchvision.datasets.folder import default_loader


class CustomCLEAR10Dataset(Dataset):
    """
    è‡ªå®šä¹‰CLEAR10æ•°æ®é›†ç±»ï¼Œç”¨äºæ”¯æŒå”¯ä¸€ç´¢å¼•å’ŒåŸŸä¿¡æ¯
    """

    def __init__(self, root=None, transform=None, data=None, targets=None, domain=None):
        self.root = root
        self.transform = transform
        self.domain = domain

        # å¦‚æœç›´æ¥æä¾›äº†æ•°æ®å’Œæ ‡ç­¾
        if data is not None and targets is not None:
            self.data = data
            self.targets = targets
            self.uq_idxs = np.array(range(len(self.targets)))
            return

        # ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½æ•°æ®
        self.data = []
        self.targets = []

        # éå†æŒ‡å®šåŸŸæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ç±»åˆ«
        domain_path = os.path.join(root, str(domain))
        for class_id, class_name in enumerate(sorted(os.listdir(domain_path))):
            class_path = os.path.join(domain_path, class_name)
            if os.path.isdir(class_path):
                for img_name in os.listdir(class_path):
                    if img_name.endswith(('.jpg', '.png', '.jpeg')):
                        img_path = os.path.join(class_path, img_name)
                        self.data.append(img_path)  # å­˜å‚¨å›¾ç‰‡è·¯å¾„
                        self.targets.append(class_id)

        self.targets = np.array(self.targets)
        self.uq_idxs = np.array(range(len(self.targets)))

    def __getitem__(self, item):
        if isinstance(self.data[item], str):  # å¦‚æœæ˜¯è·¯å¾„
            img = Image.open(self.data[item]).convert('RGB')
        else:  # å¦‚æœæ˜¯å·²åŠ è½½çš„å›¾åƒ
            img = self.data[item]

        label = self.targets[item]

        if self.transform is not None:
            img = self.transform(img)

        uq_idx = self.uq_idxs[item]

        return img, label, uq_idx

    def __len__(self):
        return len(self.targets)




def subsample_dataset(dataset, idxs):
    """
    åŸºäºç´¢å¼•å­é‡‡æ ·æ•°æ®é›†
    """
    if len(idxs) > 0:
        new_dataset = deepcopy(dataset)
        new_dataset.data = [new_dataset.data[i] for i in idxs]
        new_dataset.targets = new_dataset.targets[idxs]
        new_dataset.uq_idxs = new_dataset.uq_idxs[idxs]

        return new_dataset
    else:
        return None


def subsample_classes(dataset, include_classes=(0, 1, 2, 3, 4, 5, 6)):
    """
    ä»…ä¿ç•™ç‰¹å®šç±»åˆ«çš„æ ·æœ¬
    """
    cls_idxs = [x for x, t in enumerate(dataset.targets) if t in include_classes]

    target_xform_dict = {}
    for i, k in enumerate(include_classes):
        target_xform_dict[k] = i

    dataset = subsample_dataset(dataset, np.array(cls_idxs))

    dataset.target_transform = lambda x: target_xform_dict[x]

    return dataset

def subDataset_wholeDataset(datalist):
    """
    Combine multiple datasets into one.
    """

    if not datalist:
        return None

    wholeDataset = deepcopy(datalist[0])

    if isinstance(wholeDataset.data[0], str):  # å›¾ç‰‡è·¯å¾„
        wholeDataset.data = []
        for d in datalist:
            wholeDataset.data.extend(d.data)
    else:  # Loaded images
        all_data = []
        for d in datalist:
            all_data.extend(d.data)
        wholeDataset.data = all_data

    wholeDataset.targets = np.concatenate([d.targets for d in datalist], axis=0)
    wholeDataset.uq_idxs = np.concatenate([d.uq_idxs for d in datalist], axis=0)

    return wholeDataset


def get_clear_datasets(train_transform, test_transform, config_dict,
                          train_classes=(0, 1, 2, 3, 4, 5, 6),
                          novel_classes=(7, 8, 9),
                          prop_train_labels=1.0,
                          split_train_val=False, is_shuffle=False, seed=0,
                          test_mode='current_session'): # current_session, cumulative_session

    """
    ä¸ºCLEAR10æ•°æ®é›†åˆ›å»ºé€‚ç”¨äºContinual-GCDçš„æ•°æ®åŠ è½½å™¨

    å‚æ•°:
    - train_transform: è®­ç»ƒé›†å˜æ¢
    - test_transform: æµ‹è¯•é›†å˜æ¢
    - config_dict: é…ç½®å­—å…¸
    - train_classes: åˆå§‹è®­ç»ƒç±»åˆ«ç´¢å¼•(0-6)
    - novel_classes: å¢é‡å­¦ä¹ ç±»åˆ«ç´¢å¼•(7-9)
    - split_train_val: æ˜¯å¦åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    - is_shuffle: æ˜¯å¦æ‰“ä¹±æ–°ç±»é¡ºåº
    - seed: éšæœºç§å­

    è¿”å›:
    - é€‚ç”¨äºHappyæ¡†æ¶çš„æ•°æ®é›†å­—å…¸
    """
    # é…ç½®åœ¨çº¿ä¼šè¯å‚æ•°
    continual_session_num = config_dict.get('continual_session_num', 3)

    # ä½¿ç”¨æŒ‡å®šçš„random seed
    if seed is not None:
        np.random.seed(seed)

    # ===========================================
    # 1. ç¦»çº¿é˜¶æ®µ - æ–‡ä»¶å¤¹1å‰7ä¸ªç±»
    # ===========================================

    # åŠ è½½æ–‡ä»¶å¤¹1çš„è®­ç»ƒæ•°æ®
    domain1_train_dataset = CustomCLEAR10Dataset(
        root=os.path.join(clear_10_root, 'train'),
        transform=train_transform,
        domain=1
    )

    # æå–å‰7ä¸ªç±»ä½œä¸ºè®­ç»ƒæ•°æ®
    offline_train_dataset = subsample_classes(
        deepcopy(domain1_train_dataset),
        include_classes=train_classes
    )

    # åŠ è½½æµ‹è¯•é›† - æ–‡ä»¶å¤¹1
    test_dataset_full = CustomCLEAR10Dataset(
        root=os.path.join(clear_10_root, 'test'),
        transform=test_transform,
        domain=1
    )

    # ç¦»çº¿æµ‹è¯•é›† - åªåŒ…å«7ä¸ªæ—§ç±»
    offline_test_dataset = subsample_classes(
        deepcopy(test_dataset_full),
        include_classes=train_classes
    )

    # ===========================================
    # 2. åœ¨çº¿å¢é‡é˜¶æ®µ
    # ===========================================

    # æ¯ä¸ªä¼šè¯çš„æ–°ç±»æ˜ å°„
    session_novel_class_map = {
        0: [novel_classes[0]],  # Session 1: racing
        1: [novel_classes[0], novel_classes[1]],  # Session 2: racing, soccer
        2: list(novel_classes)  # Session 3: racing, soccer, sweater
    }

    # åˆ›å»ºåœ¨çº¿ä¼šè¯æ•°æ®é›†
    online_old_dataset_unlabelled_list = []
    online_novel_dataset_unlabelled_list = []
    online_test_dataset_list = []
    cumulative_test_datasets = [offline_test_dataset]

    for session in range(continual_session_num):
        domain_id = session + 2  # ä¼šè¯1å¯¹åº”æ–‡ä»¶å¤¹2ï¼Œä¾æ­¤ç±»æ¨(å¯¹äºå¢é‡ä»»åŠ¡æ¥è¯´è¿™é‡Œçš„sessionæ˜¯ä»0å¼€å§‹çš„ï¼Œä¹Ÿå°±æ˜¯session1)

        # âœ… é‡æ–°åŠ è½½ Online Test æ•°æ®
        test_dataset_full = CustomCLEAR10Dataset(
            root=os.path.join(clear_10_root, 'test'),
            transform=test_transform,
            domain=domain_id  # ğŸ”¥ è¿™é‡ŒåŠ¨æ€åˆ‡æ¢ `test/{session+2}/`
        )

        # å½“å‰ä¼šè¯éœ€è¦ä½¿ç”¨çš„æ‰€æœ‰ç±»åˆ«
        current_novel_classes = session_novel_class_map[session]

        # åŠ è½½å½“å‰åŸŸçš„è®­ç»ƒæ•°æ®
        domain_train_dataset = CustomCLEAR10Dataset(
            root=os.path.join(clear_10_root, 'train'),
            transform=train_transform,
            domain=domain_id
        )

        # 1. æå–æ—§ç±»æ ·æœ¬
        old_classes_dataset = subsample_classes(
            deepcopy(domain_train_dataset),
            include_classes=train_classes
        )

        # ä¸ºæ¯ä¸ªæ—§ç±»éšæœºé€‰æ‹©ã€Œå‚æ•°ã€ä¸ªæ ·æœ¬
        old_samples_list = []
        for cls in train_classes:
            cls_idxs = np.where(old_classes_dataset.targets == cls)[0]
            sample_count = config_dict['online_old_seen_num']
            if len(cls_idxs) > sample_count:
                selected_idxs = np.random.choice(cls_idxs, sample_count, replace=False)
                old_samples_list.append(subsample_dataset(deepcopy(old_classes_dataset), selected_idxs))
            else:
                old_samples_list.append(subsample_dataset(deepcopy(old_classes_dataset), cls_idxs))

        # åˆå¹¶æ‰€æœ‰æ—§ç±»æ ·æœ¬
        session_old_dataset = subDataset_wholeDataset(old_samples_list)

        # 2. æå–æ–°ç±»æ ·æœ¬
        novel_samples_list = []

        for i, novel_cls in enumerate(current_novel_classes):
            # ç­›é€‰å‡ºè¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
            novel_cls_dataset = subsample_classes(
                deepcopy(domain_train_dataset),
                include_classes=[novel_cls]
            )

            # ä¾æ®ç±»åˆ«åœ¨å½“å‰ä¼šè¯ä¸­çš„çŠ¶æ€å†³å®šé‡‡æ ·æ•°é‡
            if novel_cls == novel_classes[session]:  # å½“å‰ä¼šè¯çš„æ–°ç±»
                sample_count = config_dict['online_novel_unseen_num']
            else:  # å·²è§è¿‡çš„æ–°ç±»
                sample_count = config_dict['online_novel_seen_num']

            cls_idxs = np.where(novel_cls_dataset.targets == novel_cls)[0]
            if len(cls_idxs) > sample_count:
                selected_idxs = np.random.choice(cls_idxs, sample_count, replace=False)
                novel_samples_list.append(subsample_dataset(deepcopy(novel_cls_dataset), selected_idxs))
            else:
                novel_samples_list.append(subsample_dataset(deepcopy(novel_cls_dataset), cls_idxs))

        # åˆå¹¶æ‰€æœ‰æ–°ç±»æ ·æœ¬
        session_novel_dataset = subDataset_wholeDataset(novel_samples_list)

        # æ·»åŠ åˆ°ä¼šè¯åˆ—è¡¨
        online_old_dataset_unlabelled_list.append(session_old_dataset)
        online_novel_dataset_unlabelled_list.append(session_novel_dataset)

        # 3. åˆ›å»ºå½“å‰ä¼šè¯çš„æµ‹è¯•é›†
        # åŒ…å«æ—§ç±»å’Œå½“å‰æ‰€æœ‰å‡ºç°è¿‡çš„æ–°ç±»
        test_classes = list(train_classes) + current_novel_classes
        session_test_dataset = subsample_classes(
            test_dataset_full,
            include_classes=test_classes
        )

        # ===========================================
        # Update: 2025.3.20
        # Function: æ·»åŠ ä¸¤ç§å¢é‡è¯„ä¼°æ¨¡å¼ï¼š1.current sessionå’Œcumulative sessionï¼›
        # é€šè¿‡å‚æ•° test_mode æ§åˆ¶
        # ===========================================
        if test_mode == 'cumulative_session':
            # Session-0 -> Session-T
            cumulative_test_datasets.append(session_test_dataset)  # âœ… é€æ­¥ç´¯åŠ 
            combined_test_dataset = subDataset_wholeDataset(cumulative_test_datasets)  # âœ… åˆå¹¶
            combined.target_transform = target_transform
            online_test_dataset_list.append(combined_test_dataset)
            print("online_test_dataset_list:", len(online_test_dataset_list))
        else:
            # ä»…åŒ…å«å½“å‰ä¼šè¯(åŸŸ)çš„testç±»åˆ«
            online_test_dataset_list.append(session_test_dataset)
            print("online_test_dataset_list:", len(online_test_dataset_list))

    # ===========================================
    # 3. ç»„ç»‡è¿”å›ç»“æœ
    # ===========================================

    all_datasets = {
        'offline_train_dataset': offline_train_dataset,
        'offline_test_dataset': offline_test_dataset,
        'online_old_dataset_unlabelled_list': online_old_dataset_unlabelled_list,
        'online_novel_dataset_unlabelled_list': online_novel_dataset_unlabelled_list,
        'online_test_dataset_list': online_test_dataset_list,
    }

    # å¦‚æœéœ€è¦æ‰“ä¹±ç±»åˆ«é¡ºåº
    if is_shuffle:
        novel_targets_shuffle = list(novel_classes)
        np.random.shuffle(novel_targets_shuffle)
    else:
        novel_targets_shuffle = list(novel_classes)

    return all_datasets, novel_targets_shuffle




# æµ‹è¯•ä»£ç ï¼Œä½¿ç”¨æ—¶è¯·æ³¨é‡Šæ‰
"""
if __name__ == '__main__':
    import torchvision.transforms as transforms
    from collections import Counter

    # å®šä¹‰ç®€å•çš„ transform
    train_transform = transforms.ToTensor()
    test_transform = transforms.ToTensor()

    # åˆ›å»ºé…ç½®å­—å…¸
    config_dict = {
        'continual_session_num': 3,
        'online_novel_unseen_num': 300,
        'online_old_seen_num': 50,
        'online_novel_seen_num': 50,
    }

    # åŠ è½½å®Œæ•´ CLEAR10 æ•°æ®é›†
    datasets, novel_classes = get_clear_datasets(
        train_transform=train_transform,
        test_transform=test_transform,
        config_dict=config_dict,
        train_classes=(0, 1, 2, 3, 4, 5, 6),
        novel_classes=(7, 8, 9),
        prop_train_labels=1.0,
        split_train_val=False,
        is_shuffle=False,
        seed=0,
        test_mode='current_session' # current_session, cumulative_session
    )

    print("\n========== CLEAR10 Dataset Info ==========")

    # ç¦»çº¿è®­ç»ƒé›†
    offline_train = datasets['offline_train_dataset']
    print(f"Offline Train - Num Samples: {len(offline_train)}")
    print(f"Offline Train - Class Distribution: {Counter(offline_train.targets)}")

    # ç¦»çº¿æµ‹è¯•é›†
    offline_test = datasets['offline_test_dataset']
    print(f"Offline Test - Num Samples: {len(offline_test)}")
    print(f"Offline Test - Class Distribution: {Counter(offline_test.targets)}")

    # åœ¨çº¿é˜¶æ®µ
    for i, (old_set, novel_set, test_set) in enumerate(zip(
        datasets['online_old_dataset_unlabelled_list'],
        datasets['online_novel_dataset_unlabelled_list'],
        datasets['online_test_dataset_list']
    )):
        print(f"\n--- Session {i + 1} ---")
        print(f"Online Old Unlabelled Samples: {len(old_set)}")
        print(f"Online Novel Unlabelled Samples: {len(novel_set)}")
        print(f"Online Test Samples: {len(test_set)}")
        print(f"Online Test Class Distribution: {Counter(test_set.targets)}")
"""
if __name__ == '__main__':
    from torchvision import transforms

    dummy_transform = transforms.Compose([transforms.ToTensor()])

    config_dict = {
        'continual_session_num': 3,
        'online_novel_unseen_num': 300,
        'online_old_seen_num': 50,
        'online_novel_seen_num': 50,
    }

    all_datasets, novel_targets_shuffle = get_clear_datasets(
        train_transform=dummy_transform,
        test_transform=dummy_transform,
        config_dict=config_dict,
        is_shuffle=False
    )

    novel_datasets = all_datasets['online_novel_dataset_unlabelled_list']

    for session_id, dataset in enumerate(novel_datasets):
        print(f"\n[Session {session_id}] Online Novel Unlabelled Info")
        class_counts = {}
        for path, target in zip(dataset.data, dataset.targets):
            class_name = os.path.basename(os.path.dirname(path))
            if class_name not in class_counts:
                class_counts[class_name] = 0
            class_counts[class_name] += 1

        for class_name, count in class_counts.items():
            print(f"Class: {class_name}, Count: {count}")

