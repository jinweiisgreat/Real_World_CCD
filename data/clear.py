from PIL import Image
import os
import numpy as np
from copy import deepcopy
from torch.utils.data import Dataset
from data.data_utils import subsample_instances
from config import clear_10_root


class CustomCLEAR10Dataset(Dataset):
    """
    è‡ªå®šä¹‰CLEAR10æ•°æ®é›†ç±»ï¼Œç”¨äºæ”¯æŒå”¯ä¸€ç´¢å¼•å’ŒåŸŸä¿¡æ¯
    """

    def __init__(self, root=None, transform=None, data=None, targets=None, domain=None):
        self.root = root
        self.transform = transform
        self.domain = domain

        # å¦‚æœç›´æ¥æä¾›äº†æ•°æ®å’Œæ ‡ç­¾
        if data is not None and targets is not None:
            self.data = data
            self.targets = targets
            self.uq_idxs = np.array(range(len(self.targets)))
            return

        # ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½æ•°æ®
        self.data = []
        self.targets = []

        # éå†æŒ‡å®šåŸŸæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ç±»åˆ«
        domain_path = os.path.join(root, str(domain))
        for class_id, class_name in enumerate(sorted(os.listdir(domain_path))):
            class_path = os.path.join(domain_path, class_name)
            if os.path.isdir(class_path):
                for img_name in os.listdir(class_path):
                    if img_name.endswith(('.jpg', '.png', '.jpeg')):
                        img_path = os.path.join(class_path, img_name)
                        self.data.append(img_path)  # å­˜å‚¨å›¾ç‰‡è·¯å¾„
                        self.targets.append(class_id)

        self.targets = np.array(self.targets)
        self.uq_idxs = np.array(range(len(self.targets)))

    def __getitem__(self, item):
        if isinstance(self.data[item], str):  # å¦‚æœæ˜¯è·¯å¾„
            img = Image.open(self.data[item]).convert('RGB')
        else:  # å¦‚æœæ˜¯å·²åŠ è½½çš„å›¾åƒ
            img = self.data[item]

        label = self.targets[item]

        if self.transform is not None:
            img = self.transform(img)

        uq_idx = self.uq_idxs[item]

        return img, label, uq_idx

    def __len__(self):
        return len(self.targets)


def subsample_dataset(dataset, idxs):
    """
    åŸºäºç´¢å¼•å­é‡‡æ ·æ•°æ®é›†
    """
    if len(idxs) > 0:
        new_dataset = deepcopy(dataset)
        new_dataset.data = [new_dataset.data[i] for i in idxs]
        new_dataset.targets = new_dataset.targets[idxs]
        new_dataset.uq_idxs = new_dataset.uq_idxs[idxs]
        return new_dataset
    else:
        return None


def subsample_classes(dataset, include_classes=(0, 1, 2, 3, 4, 5, 6)):
    """
    ä»…ä¿ç•™ç‰¹å®šç±»åˆ«çš„æ ·æœ¬
    """
    cls_idxs = [x for x, t in enumerate(dataset.targets) if t in include_classes]

    target_xform_dict = {}
    for i, k in enumerate(include_classes):
        target_xform_dict[k] = i

    dataset = subsample_dataset(dataset, np.array(cls_idxs))

    return dataset


def subDataset_wholeDataset(datalist):
    """
    å°†å¤šä¸ªæ•°æ®é›†åˆå¹¶ä¸ºä¸€ä¸ª
    """
    if not datalist:
        return None

    wholeDataset = deepcopy(datalist[0])

    if isinstance(wholeDataset.data[0], str):  # å›¾ç‰‡è·¯å¾„
        wholeDataset.data = []
        for d in datalist:
            wholeDataset.data.extend(d.data)
    else:  # å·²åŠ è½½çš„å›¾åƒ
        all_data = []
        for d in datalist:
            all_data.extend(d.data)
        wholeDataset.data = all_data

    wholeDataset.targets = np.concatenate([d.targets for d in datalist], axis=0)
    wholeDataset.uq_idxs = np.concatenate([d.uq_idxs for d in datalist], axis=0)

    return wholeDataset


def get_clear_datasets(train_transform, test_transform, config_dict,
                          train_classes=(0, 1, 2, 3, 4, 5, 6),
                          novel_classes=(7, 8, 9),
                          prop_train_labels=1.0,
                          split_train_val=False, is_shuffle=False, seed=0,
                          test_mode='current_session'):
    """
    ä¸ºCLEAR10æ•°æ®é›†åˆ›å»ºé€‚ç”¨äºContinual-GCDçš„æ•°æ®åŠ è½½å™¨

    å‚æ•°:
    - train_transform: è®­ç»ƒé›†å˜æ¢
    - test_transform: æµ‹è¯•é›†å˜æ¢
    - config_dict: é…ç½®å­—å…¸
    - train_classes: åˆå§‹è®­ç»ƒç±»åˆ«ç´¢å¼•(0-6)
    - novel_classes: å¢é‡å­¦ä¹ ç±»åˆ«ç´¢å¼•(7-9)
    - split_train_val: æ˜¯å¦åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
    - is_shuffle: æ˜¯å¦æ‰“ä¹±æ–°ç±»é¡ºåº
    - seed: éšæœºç§å­

    è¿”å›:
    - é€‚ç”¨äºHappyæ¡†æ¶çš„æ•°æ®é›†å­—å…¸
    """
    # é…ç½®åœ¨çº¿ä¼šè¯å‚æ•°
    continual_session_num = config_dict.get('continual_session_num', 3)

    # ä½¿ç”¨æŒ‡å®šçš„random seed
    if seed is not None:
        np.random.seed(seed)

    # ===========================================
    # 1. ç¦»çº¿é˜¶æ®µ - æ–‡ä»¶å¤¹1å‰7ä¸ªç±»
    # ===========================================

    # åŠ è½½æ–‡ä»¶å¤¹1çš„è®­ç»ƒæ•°æ®
    domain1_train_dataset = CustomCLEAR10Dataset(
        root=os.path.join(clear_10_root, 'train'),
        transform=train_transform,
        domain=1
    )

    # æå–å‰7ä¸ªç±»ä½œä¸ºè®­ç»ƒæ•°æ®
    offline_train_dataset = subsample_classes(
        deepcopy(domain1_train_dataset),
        include_classes=train_classes
    )

    # åŠ è½½æµ‹è¯•é›† - æ–‡ä»¶å¤¹1
    test_dataset_full = CustomCLEAR10Dataset(
        root=os.path.join(clear_10_root, 'test'),
        transform=test_transform,
        domain=1
    )

    # ç¦»çº¿æµ‹è¯•é›† - åªåŒ…å«7ä¸ªæ—§ç±»
    offline_test_dataset = subsample_classes(
        deepcopy(test_dataset_full),
        include_classes=train_classes
    )

    # ===========================================
    # 2. åœ¨çº¿å¢é‡é˜¶æ®µ
    # ===========================================

    # æ¯ä¸ªä¼šè¯çš„æ–°ç±»æ˜ å°„
    session_novel_class_map = {
        0: [novel_classes[0]],  # Session 1: racing
        1: [novel_classes[0], novel_classes[1]],  # Session 2: racing, soccer
        2: list(novel_classes)  # Session 3: racing, soccer, sweater
    }

    # åˆ›å»ºåœ¨çº¿ä¼šè¯æ•°æ®é›†
    online_old_dataset_unlabelled_list = []
    online_novel_dataset_unlabelled_list = []
    online_test_dataset_list = []
    cumulative_test_datasets = [offline_test_dataset]

    for session in range(continual_session_num):
        domain_id = session + 2  # ä¼šè¯1å¯¹åº”æ–‡ä»¶å¤¹2ï¼Œä¾æ­¤ç±»æ¨

        # âœ… é‡æ–°åŠ è½½ Online Test æ•°æ®
        test_dataset_full = CustomCLEAR10Dataset(
            root=os.path.join(clear_10_root, 'test'),
            transform=test_transform,
            domain=domain_id  # ğŸ”¥ è¿™é‡ŒåŠ¨æ€åˆ‡æ¢ `test/{session+2}/`
        )

        # å½“å‰ä¼šè¯éœ€è¦ä½¿ç”¨çš„æ‰€æœ‰ç±»åˆ«
        current_novel_classes = session_novel_class_map[session]

        # åŠ è½½å½“å‰åŸŸçš„è®­ç»ƒæ•°æ®
        domain_train_dataset = CustomCLEAR10Dataset(
            root=os.path.join(clear_10_root, 'train'),
            transform=train_transform,
            domain=domain_id
        )

        # 1. æå–æ—§ç±»æ ·æœ¬
        old_classes_dataset = subsample_classes(
            deepcopy(domain_train_dataset),
            include_classes=train_classes
        )

        # ä¸ºæ¯ä¸ªæ—§ç±»éšæœºé€‰æ‹©ã€Œå‚æ•°ã€ä¸ªæ ·æœ¬
        old_samples_list = []
        for cls in train_classes:
            cls_idxs = np.where(old_classes_dataset.targets == cls)[0]
            sample_count = config_dict['online_old_seen_num']
            if len(cls_idxs) > sample_count:
                selected_idxs = np.random.choice(cls_idxs, sample_count, replace=False)
                old_samples_list.append(subsample_dataset(deepcopy(old_classes_dataset), selected_idxs))
            else:
                old_samples_list.append(subsample_dataset(deepcopy(old_classes_dataset), cls_idxs))

        # åˆå¹¶æ‰€æœ‰æ—§ç±»æ ·æœ¬
        session_old_dataset = subDataset_wholeDataset(old_samples_list)

        # 2. æå–æ–°ç±»æ ·æœ¬
        novel_samples_list = []

        for i, novel_cls in enumerate(current_novel_classes):
            # ç­›é€‰å‡ºè¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
            novel_cls_dataset = subsample_classes(
                deepcopy(domain_train_dataset),
                include_classes=[novel_cls]
            )

            # ä¾æ®ç±»åˆ«åœ¨å½“å‰ä¼šè¯ä¸­çš„çŠ¶æ€å†³å®šé‡‡æ ·æ•°é‡
            if novel_cls == novel_classes[session]:  # å½“å‰ä¼šè¯çš„æ–°ç±»
                sample_count = config_dict['online_novel_unseen_num']
            else:  # å·²è§è¿‡çš„æ–°ç±»
                sample_count = config_dict['online_novel_seen_num']

            cls_idxs = np.where(novel_cls_dataset.targets == novel_cls)[0]
            if len(cls_idxs) > sample_count:
                selected_idxs = np.random.choice(cls_idxs, sample_count, replace=False)
                novel_samples_list.append(subsample_dataset(deepcopy(novel_cls_dataset), selected_idxs))
            else:
                novel_samples_list.append(subsample_dataset(deepcopy(novel_cls_dataset), cls_idxs))

        # åˆå¹¶æ‰€æœ‰æ–°ç±»æ ·æœ¬
        session_novel_dataset = subDataset_wholeDataset(novel_samples_list)

        # æ·»åŠ åˆ°ä¼šè¯åˆ—è¡¨
        online_old_dataset_unlabelled_list.append(session_old_dataset)
        online_novel_dataset_unlabelled_list.append(session_novel_dataset)

        # 3. åˆ›å»ºå½“å‰ä¼šè¯çš„æµ‹è¯•é›†
        # åŒ…å«æ—§ç±»å’Œå½“å‰æ‰€æœ‰å‡ºç°è¿‡çš„æ–°ç±»
        test_classes = list(train_classes) + current_novel_classes
        session_test_dataset = subsample_classes(
            test_dataset_full,
            include_classes=test_classes
        )

        # ===========================================
        # Update: 2025.3.20
        # Function: æ·»åŠ ä¸¤ç§å¢é‡è¯„ä¼°æ¨¡å¼ï¼š1.current sessionå’Œcumulative sessionï¼›
        # é€šè¿‡å‚æ•° test_mode æ§åˆ¶
        # ===========================================
        if test_mode == 'cumulative_session':
            # Session-0 -> Session-T
            cumulative_test_datasets.append(session_test_dataset)  # âœ… é€æ­¥ç´¯åŠ 
            combined_test_dataset = subDataset_wholeDataset(cumulative_test_datasets)  # âœ… åˆå¹¶
            online_test_dataset_list.append(combined_test_dataset)
            print("online_test_dataset_list:", len(online_test_dataset_list))
        else:
            # ä»…åŒ…å«å½“å‰ä¼šè¯(åŸŸ)çš„testç±»åˆ«
            online_test_dataset_list.append(session_test_dataset)
            print("online_test_dataset_list:", len(online_test_dataset_list))

    # ===========================================
    # 3. ç»„ç»‡è¿”å›ç»“æœ
    # ===========================================

    all_datasets = {
        'offline_train_dataset': offline_train_dataset,
        'offline_test_dataset': offline_test_dataset,
        'online_old_dataset_unlabelled_list': online_old_dataset_unlabelled_list,
        'online_novel_dataset_unlabelled_list': online_novel_dataset_unlabelled_list,
        'online_test_dataset_list': online_test_dataset_list,
    }

    # å¦‚æœéœ€è¦æ‰“ä¹±ç±»åˆ«é¡ºåº
    if is_shuffle:
        novel_targets_shuffle = list(novel_classes)
        np.random.shuffle(novel_targets_shuffle)
    else:
        novel_targets_shuffle = list(novel_classes)

    return all_datasets, novel_targets_shuffle


# æµ‹è¯•ä»£ç ï¼Œä½¿ç”¨æ—¶è¯·æ³¨é‡Šæ‰
if __name__ == '__main__':
    test_mode = "cumulative_session"  # å¯é€‰ "current_sessionï¼Œcumulative_session"
    clear_10_root = '/home/ps/_jinwei/Dataset/CLEAR/CLEAR10_CGCD'
    config_dict = {
        'continual_session_num': 3,
        'online_novel_unseen_num': 600,
        'online_old_seen_num': 50,
        'online_novel_seen_num': 50
    }

    class_names = {
        0: 'baseball', 1: 'bus', 2: 'camera', 3: 'cosplay', 4: 'dress',
        5: 'hockey', 6: 'laptop', 7: 'racing', 8: 'soccer', 9: 'sweater'
    }

    train_classes = (0, 1, 2, 3, 4, 5, 6)
    novel_classes = (7, 8, 9)

    datasets, novel_shuffle = get_clear_datasets(
        train_transform=None,
        test_transform=None,
        config_dict=config_dict,
        train_classes=train_classes,
        novel_classes=novel_classes,
        test_mode=test_mode  # ä¼ é€’æµ‹è¯•æ¨¡å¼
    )


    # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
    def count_samples_per_class(dataset):
        class_counts = {}
        targets = dataset.targets
        for target in targets:
            class_counts[target] = class_counts.get(target, 0) + 1
        return class_counts


    # æ‰“å°æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°å’Œç±»å
    def print_class_details(class_counts, prefix=""):
        for cls, count in sorted(class_counts.items()):
            print(f"{prefix}ç±»åˆ« {cls} ({class_names[cls]}): {count} ä¸ªæ ·æœ¬")


    print("\n======== æ•°æ®é›†è¯¦ç»†ç»Ÿè®¡ ========\n")

    # ===== ç¦»çº¿é˜¶æ®µæ•°æ®é›†ç»Ÿè®¡ =====
    print("===== ç¦»çº¿é˜¶æ®µ =====")

    # ç¦»çº¿è®­ç»ƒé›†
    offline_train_counts = count_samples_per_class(datasets['offline_train_dataset'])
    print("\nç¦»çº¿è®­ç»ƒé›†:")
    print(f"æ€»æ ·æœ¬æ•°: {len(datasets['offline_train_dataset'])}")
    print_class_details(offline_train_counts, "  ")

    # ç¦»çº¿æµ‹è¯•é›†
    offline_test_counts = count_samples_per_class(datasets['offline_test_dataset'])
    print("\nç¦»çº¿æµ‹è¯•é›†:")
    print(f"æ€»æ ·æœ¬æ•°: {len(datasets['offline_test_dataset'])}")
    print_class_details(offline_test_counts, "  ")

    # ===== åœ¨çº¿é˜¶æ®µæ•°æ®é›†ç»Ÿè®¡ =====
    session_novel_class_map = {
        0: [novel_classes[0]],
        1: [novel_classes[0], novel_classes[1]],
        2: list(novel_classes)
    }

    for i in range(len(datasets['online_old_dataset_unlabelled_list'])):
        print(f"\n===== åœ¨çº¿é˜¶æ®µ - ä¼šè¯ {i + 1} =====")

        # æ—§ç±»æ— æ ‡ç­¾æ ·æœ¬
        old_dataset = datasets['online_old_dataset_unlabelled_list'][i]
        old_counts = count_samples_per_class(old_dataset)
        print("\næ—§ç±»æ— æ ‡ç­¾æ ·æœ¬:")
        print(f"æ€»æ ·æœ¬æ•°: {len(old_dataset)}")
        print_class_details(old_counts, "  ")

        # æ–°ç±»æ— æ ‡ç­¾æ ·æœ¬
        novel_dataset = datasets['online_novel_dataset_unlabelled_list'][i]
        novel_counts = count_samples_per_class(novel_dataset)

        # åŒºåˆ†å·²è§æ–°ç±»å’Œå½“å‰æ–°ç±»
        current_novel_classes = session_novel_class_map[i]
        previously_seen = []
        newly_introduced = []

        for cls in current_novel_classes:
            if i > 0 and cls in session_novel_class_map[i - 1]:
                previously_seen.append(cls)
            else:
                newly_introduced.append(cls)

        print("\næ–°ç±»æ— æ ‡ç­¾æ ·æœ¬:")
        print(f"æ€»æ ·æœ¬æ•°: {len(novel_dataset)}")

        if previously_seen:
            print("\n  å·²è§æ–°ç±»:")
            for cls in previously_seen:
                if cls in novel_counts:
                    print(f"  ç±»åˆ« {cls} ({class_names[cls]}): {novel_counts[cls]} ä¸ªæ ·æœ¬")

        if newly_introduced:
            print("\n  å½“å‰æ–°ç±»:")
            for cls in newly_introduced:
                if cls in novel_counts:
                    print(f"  ç±»åˆ« {cls} ({class_names[cls]}): {novel_counts[cls]} ä¸ªæ ·æœ¬")

        # æµ‹è¯•é›†
        test_dataset = datasets['online_test_dataset_list'][i]
        test_counts = count_samples_per_class(test_dataset)
        print(f"\næµ‹è¯•é›† (æ¨¡å¼: {test_mode}):")
        print(f"æ€»æ ·æœ¬æ•°: {len(test_dataset)}")
        print_class_details(test_counts, "  ")

